{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font size=20><b>Bayesian Classification</b></font>(In study)\n",
    "\n",
    "Simple Bayesian classifications assume that the effect of a feature value on a class is independent of other features. This assumption is called the conditional independence of the class and it simplifies the calculations.[1]\n",
    "\n",
    "Here, using Bayes theorem, we mean by P(H|X) the probability that the tuple X satisfies the assumption H. In other words, we want to calculate the probability P(H: tuple X belongs to class $C_i$. | observed values of X), To simplify the notaion, We denote this expression by $P(C_i|X)$. and:<br/><br/>\n",
    "\n",
    "(1) - $P(C_i|X)=\\frac{P(C_i\\cap X)}{P(X)}=\\frac{P(C_i|X)P(C_i)}{P(X)}$\n",
    "\n",
    "We can calculate the probability of $P(X), P(C_i)$ and $P(X|C_i)$ using the data set. Suppose  the number of features is $k$ and $X=(x_1,x_2,...,x_k)$, the number of samples is $n$, and the number of classes is $m$, then:\n",
    "\n",
    "$P(C_i) =\\frac{|C_i|}{n}$ , $P(X) =\\frac{|X|}{n}$\n",
    "\n",
    "Because $P(X)$ is the same for all classes, in practice we can skip its calculation, this will reduce the calculation time and accelerate the learning of the model. A large data set causes heavy calculations to calculate $P(X|C_i)$, To reduce these calculations, we use the conditional independence of the class and assume that there is no dependence between the features, and therefore:\n",
    "\n",
    "$P(X|C_i)=P(x_1|C_i)\\times P(x_2|C_i)\\times...\\times P(x_k|C_i)=\\prod_{j=1}^{k}{P(x_j|C_i)}$\n",
    "\n",
    "If feature j is discrete or categorical, the number of tuples that are in class $C_i$ and their $j$-th feature is equal to $x_j$ ,is divided by $|C_i|$\n",
    "\n",
    "If the attribute xj is continuous, it is usually assumed to have a Gaussian distribution with mean $\\mu$ and standard deviation $\\sigma$, which is introduced as follows: $g(x,\\mu,\\sigma)=\\frac{e^\\frac{{(x-\\mu)}^2}{2\\sigma^2}}{\\sqrt{2\\pi\\sigma}}$\n",
    "\n",
    "So, $g(x_j,\\mu_{C_i},\\sigma_{C_i})=P(x_j|C_i)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Predicting class of X</b>\n",
    "\n",
    "To specify class $X$, the goal is to find the maximum of $P(Ci|X)$ from (1):<br/>\n",
    "So that $P(C_i|X)>P(C_j|X) for \\le j \\le m,j \\neq i$\n",
    "\n",
    "For this, we do not need to calculate $P(X)$ because it is the same for all classes, also this can lead to reduced calculations. So the Bayes classifier considers $X$ to be in class $C_i$ if and only if:\n",
    "\n",
    "$P(X|C_i)P(C_i)>P(X|C_j)P(C_j) for 1\\le j \\le m, j\\neq i$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Refrences:</b><br/>\n",
    "1 - Data mining:concepts and techniques, 3rd Ed, Han.Jiawei\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
