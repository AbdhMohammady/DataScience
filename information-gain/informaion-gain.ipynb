In this notebook, we are going to implement the method to calculate information gain based on entropy and Gini method.
This is an educational notebook and I request the readers to report any bugs to <b>AbdhMohammady@gmail.com</b> or leave comment on my github <b>https://github.com/AbdhMohammady</b>.
I tried to write this notebook so that it can be used for other datasets, currently this notebook can be used for datasets with categorical columns and two-class for target column.
At the end of the notebook, after obtaining the highest information gain,you can use this column as the root of the decision tree algorithm.

The entropy of the original dataset is calculated based on the target column named 'Buy' and this column has two class 'Yes' and 'No'.

Where :

 D is our dataset 
 
 number of rows in the dataset is n(dataset)
 
 frequency of class ‘i’ in the target column is n(class i)
 
 then probability of class i in the dataset is :
$$p(class i) = {p}_{i}=\frac{{n}({class} {i})}{{n}({dataset})}$$

and entropy of dataset is :
$${Entropy}\left({D}\right)=\ -\sum_{{i}=\mathbf{1}}^{{m}}{{p}_{i}{Ln}({p}_{i})}$$


Suppose A is a column of the dataset and Aj, j:1... k are k the categories in this column.
for each Aj, suppose pi is probability of class i in the Aj then:

$${Entropy}\left({A}_{j}\right)=\ -\sum_{{j}=\mathbf{1}}^{{k}}{{p}_{i}({A}_{j}){Ln}({p}_{i}({A}_{j}))}$$

and if p(Aj) is probability of Aj in the A, then entropy of A is :

$${Entropy}\left({A}\right)=\ \sum_{{j}=\mathbf{1}}^{{m}}{{p}({A}_{j}){Entropy}({A}_{j})}$$

And as final result, information gain of A is:

$${Gain}({A})\ =\ {Entropy}({D})-\ {Entropy}({A})$$

This calculation is repeated for all columns, and at the end, the column with the highest value of information gain is selected as the root of the decision tree algoritm. This process is implemented in the below.


This article will be updated.

contact me:

https://github.com/AbdhMohammady

abdhmohammady@gmail.com

import pandas # to working with dataset
import math   # to working with math functions like log10

#This is a very sample dataset to learn Gain information rules
df = pandas.read_csv('Gain-Information-sample.csv')

# number of record we read by pandas from our dataset, 
# we use this value as size of the sample to claculte probability of each class
n_records = len(df)

# display all data
df.head()


group_object = df.groupby(by="Buy")

buy_groups_dict = group_object.groups

print("Buy groups dictionary:\n",buy_groups_dict)

group_object.describe()


#Compute entropy of the dataset

#Calculate the frequency of each item in the group

dataset_entropy = 0
dataset_gini = 0
target_probs = dict()

for key in buy_groups_dict:
    #list of real index of items in the dataset
    index_list = buy_groups_dict.get(str(key)).to_list()
    
    n_class_item = len(index_list)
    
    prob_class_item = n_class_item/n_records
    
    target_probs[key] = prob_class_item**2
    
    log_class_item = math.log(prob_class_item)
    
    dataset_entropy = dataset_entropy + prob_class_item*log_class_item
    
dataset_entropy = - dataset_entropy

dataset_gini = 1- sum(target_probs.values())

print("Dataset Entropy : ",dataset_entropy)

print("Dataset Gini    : ",dataset_gini)
    

Computing entropy of a column

# get list of column name
columns = list(df.columns.values)

# no need target column to calculate entropy and gain information
del columns[4] 

columns


# initalize empty dictionary to store 'Gain Information' of each column 
info_dict = dict()

column_entropy = 0

for column_ in columns:
    
    column_entropy = 0
    column_gini    = 0
    
    group_object = df.groupby(by=column_)

    #the categories of the column
    classes = list(group_object.groups.keys())

    for class_ in classes:

        n_yes = 0
        n_no  = 0
        
        for i in range(n_records):
            if df[column_][i]==class_:
                if df['Buy'][i]=='Yes': n_yes+=1
                if df['Buy'][i]=='No' : n_no+=1

        #Number of subclass items
        n_class = n_yes + n_no

        p_yes = 0
        
        if n_yes != 0 and n_class != 0 :p_yes = n_yes/n_class
        
        p_no  = 0
        
        if n_no != 0 and n_class != 0 :p_no = n_no/n_class
        
        class_entropy = 0
        
        if p_yes != 0 : class_entropy = p_yes*math.log(p_yes)
        
        if p_no != 0 : class_entropy += p_no*math.log(p_no)
        
        class_gini = 1- (p_yes**2 + p_no**2)
        
        class_entropy = - class_entropy

        column_entropy += (n_class/n_records)* class_entropy
        
        column_gini += (n_class/n_records)*class_gini
        
    # Information gain and gain ratio of current column
    # firts element of the value list is gain info
    # secund element of the value list is gain ratio
    # Third element of the value list is gini info
    gain__info = dataset_entropy - column_entropy
    gain_ratio = gain__info/column_entropy
    gini_info  = dataset_gini - column_gini
    info_dict[column_] = [gain__info,gain_ratio,gini_info]
    
    print("\n(GainInfo,GainRatio,Gini) = (",gain__info,",",gain_ratio,",",gini_info,")")
    

#find item with maximom value in the dictionary using Gain Information
max_item = list(info_dict.keys())[0]

for item in info_dict:
    if info_dict[item][0]> info_dict[max_item][0]:
        max_item = item

print("The column with the highest information gain:\n\n\tColumn : '"
      + max_item+"'\n\tStandard method :",info_dict[max_item][0],
     "\n\tGain Ratio method:",info_dict[max_item][1],
     "\n\tGini method:",info_dict[max_item][2])

<b>Now the column found can be used for the root of the decision tree algorithm</b>
